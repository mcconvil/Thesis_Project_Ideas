# Thesis Project Ideas

My main areas of expertise are survey statistics and statistical/machine learning.  In my own research, I adapt statistical learning techniques, such as lasso, penalized splines, and regression trees, to handle data collected under a complex sampling design (think stratification and clustering).  

Many of the projects listed below have been generated by my work with the [US Forest Inventory and Analysis Program](https://www.fia.fs.fed.us/) (FIA) and the [US Bureau of Labor Statistics](https://www.bls.gov/) (BLS) and therefore include stakeholders who would have a vested interest in your work and findings!  

* **Prediction of rare events**
    + In my work with FIA, we are trying to detect annual changes in the landscape, such as land being converted from forest to agriculture.  Unfortunately, this is a difficult classification problem because from one year to the next most land doesn't change.  This extreme class imbalance greatly hinders the effectiveness of the predictive models since a model which predicts all plots of land will have no change has a high accuracy rate but isn't very useful.  Many methods have been proposed to remedy this issue: over/undersampling, penalized methods, cost-sensitive algorithms.  Which method would work best for landscape change?  One vision for this project would be to learn and write about the various remedies for class imbalance and to conduct an empirical analysis of their applicability to forestry data. 
    + A good starting point would be He and Garcia's 2009 paper [Learning from Imbalanced Data](https://ieeexplore.ieee.org/document/5128907/).

* **Evaluating Online Nonprobability Samples**
    + Weilding randomness appropriately is one of a statistician's best super powers.  Designing a study with random assignment allows the researchers to potentially conclude a causal relationship.  Taking a random sample of 1000 people can allow a statistician to draw conclusions about millions.  But "taking a random sample" is becoming harder and harder to do. Federal survey agencies budgets are shrinking and response rates of those randomly selected individuals is ever in decline.  Therefore, survey organizations are turning to non-random samples to draw conclusions about the population.  But doing so is a difficult and delicate task.  How do we ensure that the sample is still representative of the population?   
    + A good starting point would be this [Pew study](https://www.pewresearch.org/methods/2016/05/02/evaluating-online-nonprobability-surveys/)
    + Here's also a [summary report](https://academic.oup.com/jssam/article-abstract/1/2/90/941418) of the subject

* **Random Forest Estimation for Correlated Data**

    
* **Statistical learning techniques**
    + Is there a statistical learning technique that you want to better understand? 
    + A good starting point is the [ISLR textbook](http://www-bcf.usc.edu/~gareth/ISL/).

* **Survey analysis**
    + Want to learn how to analyze data that comes from a complex sampling design?
    
* **Conducting a proper survey**    
    + Want to construct a survey, implement a sampling design and then analyze the data?  For an example, see [my paper on the impacts of voter id laws](https://www.tandfonline.com/doi/full/10.1080/2330443X.2017.1407721).


    
